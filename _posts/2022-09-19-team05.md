---
layout: post
comments: true
title: 2 vs 2 soccer game
author: Linqiao Jiang, Qi Li, Huizhuo Yuan (Team 05)
date: 2022-10-24
---

> In this project we are going to investiage **self-play for competitive Multi-agent Reinforcement Learning (MARL)**. We will explore the power of self-play on stablizing multi-agent training on several settings including soccer game, Multi-agent tennis, competitive Atari Game Pone, competitive Car-racing, etc. Among them, we build a 2 vs 2 soccer game in Unity and try different MARL algorithms on it. We provide the source code and detailed analysis.

<!--more-->
{: class="table-of-content"}

* TOC
{:toc}

---

## What is MARL?
In this project, we investigate **self-play for competitive Multi-agent Reinforcement Learning (MARL)**. Different from single-agent systems, where a stationary environment is possible, the evolution of the environmental state and the reward function that each agent received are not only determined by the envrionment, but also other agents' joint actions. As a result, agents need to take into account and interact with not only the environment but also other intelligent agents (see Fig. 1). In a multi-agents scenario, there could be cooperation (e.g., May and Cody in video game *It Takes Two*) and competition (e.g., most of board games).

<figure align="center">
  <img width="90%" src="../../../assets/images/team05/MARL.png">
  <figcaption>Fig 1. Single-agent vs Multi-agent [1].</figcaption>
</figure>

Competitive multi-agent algorithms are widely applicable, with particular interest in a variety of board, sports, or computer games including Chess, Mahjong, Soccer, Starcraft, etc. However, training agents to perform complex tasks requires high complexity of the environment, which is in general hard to achieve in regular training environments. Self-play is a concept that can be dated back to TD-gammon and has been explored to fit real tasks in AlphaGo, Dota2, etc. In a self-play game, the opponent agent are providing sufficient diversified responses, which resolves the issue of the lack of complexity in the environment. Moreover, the opponent agent are providing the trained agent with the right curriculum to learn.




## Related Work and Methodology

### PPO
For policy gradient algorithms, the update on the policy parameters equals to taking approximate gradient on an objective function, which is done by calculating a weighted gradient ascent:

$$
\nabla_\theta J(\theta)=E_{s \sim \rho^\pi, a \sim \pi_\theta}\left[\nabla_\theta \log \pi_\theta(a \mid s) Q^\pi(s, a)\right].
\tag{1}
$$

Proximal Policy Gradient (PPO) is designed to take cautious steps to maximize the improvement. While a previous work TRPO uses complex second-order methods to solve this problem, PPO is first-order method that enables the use of gradient descent. This allows for more efficient optimization and helps to avoid performance collapse. A truncated version of the PPO objective are illlustrated as follows:


$$
\min \left(\frac{\pi_\theta(a \mid s)}{\pi_{\theta_k}(a \mid s)} A^{\pi_{\theta_k}}(s, a), \operatorname{clip}\left(\frac{\pi_\theta(a \mid s)}{\pi_{\theta_k}(a \mid s)}, 1-\epsilon, 1+\epsilon\right) A^{\pi_{\theta_k}}(s, a)\right).
$$


[MAPPO](https://arxiv.org/pdf/2103.01955.pdf) (Multi-Agent Proximal Policy Optimization) is an actor-critic algorithm in which the critic learns a joint state value function. MAPPO can utilise the same training batch of trajectories to perform several update epochs.



<figure align="center">
  <img width="90%" src="../../../assets/images/team05/mappo.png">
  <figcaption>Fig 2. Architecture of Parameter Sharing paradigm.</figcaption>
</figure>


### DDPG
The deterministic policy gradient (DPG) is based on continuous and deterministic policy hypothesises $\mu_{\theta}(s): \mathcal{S} \rightarrow \mathcal{A}$ (in comparison with $\pi_{\theta}(s\mid a)$ which is a probability distribution). Instead of using (1), the objective gradient of DPG yields:
$$
\nabla_\theta J(\theta)=E_{s \sim \beta}\left[\left.\nabla_\theta \mu_\theta(s) \nabla_a Q^\mu(s, a)\right|_{a=\mu_\theta(s)}\right].
\tag{2}
$$

Based on DPG formular (2), DDPG is an actor-critic based algorithm that trains a value network and a policy network, and leverages DQN training techniques such as the experience replay, and the target network. Next we introduce adding a multi-agent structure on top of DDPG.

### MADDPG

One major challenge of using traditional reinforcement learning (RL) techniques in multi-agent settings is that the constant learning and improvement of each agent's strategy interferes with the environment and makes it highly unstable from the perspective of each individual agent. This instability violates the convergence conditions of traditional RL algorithms, making RL algorithms difficult to use in multi-agent scenarios. Furthermore, policy gradient-based algorithms also suffer from high variance in multi-agent environments, which is exacerbated by the increasing number of agents. To address these challenges, the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm has been designed with specific characteristics to improve its performance in multi-agent settings.

In 2017, OpenAI introduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in their paper "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments". [MADDPG](https://arxiv.org/pdf/1706.02275.pdf) (Multi-Agent Deep Deterministic Policy Gradient) is a variation of the DDPG algorithm for MARL, extending DDPG into a multi-agent policy gradient algorithm where **decentralized agents** learn a **centralized critic** based on the observations and actions of all agents.  The actor is conditioned on the history of local observations, while the critic is trained on the joint observation and action to approximate the joint state-action value function. Each agent individually minimizes the deterministic policy gradient loss. After training is completed, only the local actors are used in the execution phase, acting in a decentralized manner. The action space of MADDPG must be continuous due to the differentiability of actions with respect to the parameters of the actor. After, Lowe et al. apply the Gumbel-Softmax trick to learn in discrete action spaces.

<figure align="center">
  <img width="90%" src="../../../assets/images/team05/maddpg.png">
  <figcaption>Fig 3. Architecture of MADDPG. [2]</figcaption>
</figure>


Specifically, the "**decentralized actor**" updates are similar as that used in DDPG (2), we formulate it as follows:
$$
\nabla_{\theta_i} J\left(\mu_i\right)=E_{x, a \sim D}\left[\left.\nabla_{\theta_i} \mu_i\left(a_i \mid o_i\right) \nabla_{a_i} Q_i^\mu\left(x, a_1, \cdots, a_n\right)\right|_{a_i=\mu_i\left(o_i\right)}\right]
$$
or 
$$\nabla_{\theta_i} J\left(\theta_i\right)=E_{s \sim \rho^\pi, a_i \sim \pi_i}\left[\nabla_{\theta_i} \log \pi_i\left(a_i \mid o_i\right) Q_i^\pi\left(x, a_1, \cdots, a_n\right)\right]$$
for the discrete case.

In contrast to the $Q^\pi$ in (1) and $Q^\mu$ in (2), policy gradient objective in MADDPG are trained with the guidence of the centralized critic $Q_i^\pi$ or $Q_i^\mu$, which provides information of the whole system.

On the other hand, the "**centralized critic**" is trained via

$$\mathcal{L}\left(\theta_i\right)=\mathbb{E}_{\mathbf{x}, a, r, \mathbf{x}^{\prime}}\left[\left(Q_i^{\boldsymbol{\mu}}\left(\mathbf{x}, a_1, \ldots, a_N\right)-y\right)^2\right],\\ y=r_i+\left.\gamma Q_i^{\boldsymbol{\mu}^{\prime}}\left(\mathbf{x}^{\prime}, a_1^{\prime}, \ldots, a_N^{\prime}\right)\right|_{a_j^{\prime}=\boldsymbol{\mu}_j^{\prime}\left(o_j\right)}$$

With the **centralized critic** that utilizes global information, the training becomes more stable even if each agents are acting in a non-stationary manner.

### MA-SAC

[MA-SAC](https://openreview.net/pdf?id=S1ef6JBtPr) (Multi-agent Soft Actor-Critic) supports efficient off-policy learning and addresses credit assignment problem partially in both
discrete and continuous action spaces. MASAC uses the optimization goal of maximum entropy to make the algorithm more stable.  Each agent in MASAC uses an independent critic network to calculate the Q value of all states and actions, making the algorithm can be applied in a mixed collaborative and competitive environment

As shown in Figure 1, the agent $i$ in MASAC has 4 deep neural networks, which are actor network, critic network, target actor network, and target critic network. In the training process, only the actor network and critic network are trained. The target actor network and target critic network are used to stabilizing the learning effect of the actor network and critic network. The actor network and target actor network respectively utilize the current observation $o_i$ of the agent and the observation of the next state ${o_i}^{'}$ to generate the current action and target action. 

<figure align="center">
  <img width="90%" src="../../../assets/images/team05/ma-sac.png">
  <figcaption>Fig 4. Architecture of MA-SAC. [3]</figcaption>
</figure>

The input of critic network is the observation $x$ and the action $a$ of all the current agents, and the output is the Q value of agent $i$ action, $Q_i$. The input of target critic network is the observation $x'$ and action $a'$ of the agent in the next state, and the output is the Q value of agent $i$ 's target action, $T_{Q_i}$. Meanwhile, every time the parameters of actor network and critic network are updated, it will soft update target actor network and target critic network ensuring the stable operation of the algorithm.



### MA-POCA
[MA-POCA](https://arxiv.org/pdf/2111.05992.pdf) (Multi-Agent POsthumous Credit Assignment) is a neural network that acts as a "coach" for a whole group of agents. Since Muti-Agent game needs to be considered with functionality for training cooperative behaviors - i.e., groups of agents working towards a common goal, where the success of the individual is linked to the success of the whole group. In such a scenario, agents typically receive rewards as a group. You can give rewards to the team as a whole, and the agents will learn how best to contribute to achieving that reward. Agents can also be given rewards individually, and the team will work together to help the individual achieve those goals. 

<figure align="center">
  <img width="90%" src="../../../assets/images/team05/poca.png">
  <figcaption>Fig 5. three actors select actions based on the observations of a single agent, while a single critic evaluates the behavior of the whole group for training. [4].</figcaption>
</figure>

During an episode, agents can be added or removed from the group, such as when agents spawn or die in a game. If agents are removed mid-episode (e.g., if teammates die or are removed from the game), they will still learn whether their actions contributed to the team winning later, enabling agents to take group-beneficial actions even if they result in the individual being removed from the game (i.e., self-sacrifice). MA-POCA can also be combined with self-play to train teams of agents to play against each other.






## Challenges and Solution

We implement a toy 2 vs 2 soccer game in Unity (as shown in Fig. 2) and investigate the performance of self-play. In this game, there are two teams with two agents in each. The goal is to get the ball into the opponent's goal while preventing the ball from entering own goal.

<figure align="center">
  <img width="80%" src="../../../assets/images/team05/soccer.png">
  <figcaption>Fig 2. 2 vs 2 soccer game in Unity.</figcaption>
</figure>

Now, we have the following question:

- How to control the role separately?
  
A challenge that we are facing in training for the two-soccer game is that we need to train more than one brain at a time, e.g. there is a goalie that needs a defensive brain and a striker that needs an offensive brain. So we need different reward functions to train different controllers. By enabling the multi-brain training feature in the ML-Agents toolkit of Unity, we are building our algorithms based on not only a two-player setting, but also each player has two brains present. After training, we get one neural network model for each brain, that further enables mixing and matching different hyperparameters.

- How to train the antagonism between both sides?
  
Since there could exist a situation that two-side players do not move all the time that making the game meaningless, it is important to consider how to train the adversarial between two players, we propose to experiment with using the self-play technique to enhance the performance. We will analyze how the two-brain setting affects the training, as well as how the self-play affects the training.





## Unity Environment Setup (borrowed from [here](https://github.com/bryanoliveira/soccer-twos-env))

The environment is based on Unity ML Agents' [Soccer Twos](https://github.com/Unity-Technologies/ml-agents/blob/92ff2c26fef7174b443115454fa1c6045d622bc2/docs/Learning-Environment-Examples.md#soccer-twos), so most of the specs are the same. Here, four agents compete in a 2 vs 2 toy soccer game, aiming to get the ball into the opponent's goal while preventing the ball from entering own goal.

- **Goal**: Get the ball into the opponent's goal while preventing the ball from entering own goal.
  
- **Agent Reward Function**:
  - `1 - accumulated time penalty`: when ball enters opponent's goal. Accumulated time penalty is incremented by `(1 / MaxSteps)` every fixed update and is reset to 0 at the beginning of an episode. In this build, `MaxSteps = 5000`.
  - `-1`: when ball enters team's goal.

- **Observation space**: 336 corresponding to 11 ray-casts forward distributed over 120 degrees (264) and 3 ray-casts backward distributed over 90 degrees each detecting 6 possible object types, along with the object's distance. The forward ray-casts contribute 264 state dimensions and backward 72 state dimensions.

- **Action space**: 3 discrete branched actions (MultiDiscrete) corresponding to forward, backward, sideways movement, as well as rotation (27 discrete actions).

<div align="center">
    <img src="https://raw.githubusercontent.com/bryanoliveira/soccer-twos-env/main/images/obs.png" width="600"/>
</div>
<br/>



## Experiments & Results


### PPO [TODO: training curve & analysis]

### MADDPG [TODO: training curve & analysis]

### SAC [TODO: training curve & analysis]

 


### MAPOCA

In the next two fitures we present our training curve of the POCA algorithm on the two-player soccer game environment. Figure 3 shows the training error during training and Figure 4 shows the group cumulative rewards across the training process.

<figure align="center">
  <img width="80%" src="../../../assets/images/team05/poca_ep_len.png">
  <figcaption>Fig 3. episode length.</figcaption>
</figure>

<figure align="center">
  <img width="80%" src="../../../assets/images/team05/poca_group_cumulative_r.png">
  <figcaption>Fig 4. group cumulative reward.</figcaption>
</figure>

In the video below, we demonstrate the behavior of our trained agent. We see that it follows similar behavior as the example video above. The goalie stays around the gate to defend when the ball gets close to the gate, and the striker moves intensively across the field. However, we notice that the role of the goalie and the striker is switching during an episode, meaning that sometimes the goalie moves forward to shoot. We anticipate that this is due to improper assignment of goals for the defenser and the attacker. We will try solving this issue and improving the performance in the upcoming weeks.

<p align="center">
	<iframe width="618" height="473" src="https://www.youtube.com/embed/m-LZjAXWJ5w" frameborder="0" allowfullscreen ng-show="showvideo"></iframe>
</p>








### Self-play (optional)






## Fight time

### PPO v.s. POCA [TODO: training curve? & analysis]

<p align="center">
	<iframe width="618" height="473" src="https://www.youtube.com/embed/6U-o13vdLSk" frameborder="0" allowfullscreen ng-show="showvideo"></iframe>
</p>


### Find the two better model to compete [TODO: training curve? & video & analysis]






## Conclusion [TODO]

In this project, we provided a easy-to-use multi-agent environment and plug-and-play MARL algorithms to class and, also give detailed analysis of behaviors of MARL agents to readers.



## Future Work

- Design a new reward function to improve the performances of the MARL models

- Create a GUI to allow users to flight with RL agents.

- Try more algorithms 
  - Value Decomposition
    - [QTRAN](https://arxiv.org/abs/1905.05408): Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning. [5]
  - Communication
    - [IC3Net](https://arxiv.org/abs/1812.09755)：Learning when to Communicate at Scale in Multiagent Cooperative and Competitive Tasks. [6]

- Deploy in more environments. 
  - [Multi-agent tennis](https://github.com/kantologist/multiagent-sac)
  - [Competitive pone](https://github.com/ucla-rlcourse/competitive-rl)
  - [Competitive car-racing](https://github.com/ucla-rlcourse/competitive-rl)
  
- Finetune the hyper-parameters and analyze why some algorithms will fail.



## Source code

Our code is available at [https://github.com/liqi0126/soccer-twos](https://github.com/liqi0126/soccer-twos).



## References

1. Yang, Yaodong, and Jun Wang. “An Overview of Multi-Agent Reinforcement Learning from Game Theoretical Perspective.” ArXiv.org, 18 Mar. 2021, https://arxiv.org/abs/2011.00583. 
2. Lowe, Ryan, et al. “Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments.” ArXiv.org, 14 Mar. 2020, https://arxiv.org/abs/1706.02275. 
3. Shuo,Zhen-zhen, et al. “Deep Reinforcement Learning Algorithm of Multi⁃Agent Based on SAC.” ACTA ELECTONICA SINICA, 25 Sept. 2021, https://www.ejournal.org.cn/EN/10.12263/DZXB.20200243. 
4. Cohen, Andrew, et al. “On the Use and Misuse of Absorbing States in Multi-Agent Reinforcement Learning.” ArXiv.org, 7 June 2022, https://arxiv.org/abs/2111.05992. 
5. Son, Kyunghwan, et al. “QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning.” ArXiv.org, 14 May 2019, https://arxiv.org/abs/1905.05408. 
6.  Singh, Amanpreet, et al. “Learning When to Communicate at Scale in Multiagent Cooperative and Competitive Tasks.” ArXiv.org, 23 Dec. 2018, https://arxiv.org/abs/1812.09755. 
